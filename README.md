# Project 4: NumC (Public)

**Note: I cannot release the source code for this project because it is against course policy.**

## Overall Process
I started off by implementing the naive implementations of the six matrix arithmetic operations. As these were fairly straightforward, I moved on to the allocate_matrix, where I had to think about the relationship between parent and child matrices. I implemented my reference matrices with path compression so that all child matrices would directly point to the original parent data. I linked my matrix operation functions in setup.py, and then began writing the Python-C interface. Here, I made the design decision to handle the different types of errors that I would be required to throw. Most of my time went into reading the Python/C API Reference, which I found to be hard to navigate. In the end, I found all of the functions I needed to use and wrote the necessary wrapper functions such that Python and C could interface. One thing of note is that I made the design decision of creating helper functions for my subscripting functions. The main reason for this, other than readability, was that set_subscript had to modify values of 1x1 matrices, but in my previous implementation I returned the integer/float value if the result matrixâ€™s shape was a 1x1. I thought about the many different methods of solving this, but I decided that the solution with the least overhead was the one where I add a flag to each of my subscript helper functions such that if the flag was set to true, it would always return a matrix, otherwise it would perform the default behavior. This made implementing set_subscript very easy, and I were able to achieve the desired behavior without repeating my code. At this point, I shifted my attention to testing, and I set up a testing framework that made it extremely quick and easy to generate tests. I also modified the Makefile to make it easy to run tests. I tested each of the simple functions, as well as the other functionality that I implemented in Task 3 (such as set_subscript). In addition to finding and squashing many bugs, I set up the testing framework to run multiple tests and average the speedups so that I could have more accurate measure of my speedup results while I worked on optimization.

## Optimization
I moved on to Task 4, which by far took the longest time out of all the tasks. Again, I started off with the simple operations and applied SIMD instructions, Open MP, and loop unrolling. I did these one by one, to see what the speedups were at each stage. One problem I had early on was that the vectorized functions would cause my tests focusing on small and medium matrices to suffer drastic performance hits. I realized that more tail case use and the overhead of setting up parallelism and SIMD instructions were the root causes of this. Thus, I tested to find the ideal threshold at which if I were above, I would use SIMD and OpenMP, and if I were below, I would use the naive method. I also ran many tests to find what amount of loop unrolling was optimal for each of the different functions. 

### Simple Operations
For my element-wise addition, subtraction, absolute value, and negation functions, I used SIMD instructions, unrolling, and OpenMP to speed them up. Since I used 1 dimensional arrays to store my matrix data, this process was very straightforward and easily parallelized. For my abs and neg functions, I used bit masking to toggle the sign bit of my values by using the bitwise XOR of -0.0 (0b1000...0000 in binary) for negation and the bitwise AND of the complement of -0.0 (0b0111...1111 in binary) for absolute value. I tried using hex values for this at first (0x7FFF...) but quickly realized after testing that I were getting incorrect results. This was because my double assignment was casting the hex as a decimal value instead of as the bits it represents. I struggled to set the value explicitly since 0x7FFF... is a NaN value, and I eventually tried importing the C math library using nan("0x7FFF..."). This caused problems with my test cases, so after some research, I found an "andnot" and "xor" simd function that allowed us to use the explicit value -0.0 as my mask instead.

### Multiplication
For mul, I first thought about the algorithm itself, specifically which one to choose for the best performance and which one would be easiest to convert into SIMD instructions. I realized that the naive method was not ideal because it did not take advantage of spatial locality. I found two ways to fix this issue, either transpose matrix B such that the iteration would happen over contiguous spaces in memory, or multiply each element in matrix A with each row of matrix B. I decided to attempt the transpose method first, and used what I learned from the labs to write a transpose function and multiply matrices while taking advantage of spatial locality. While this did improve the performance of multiply, I had a difficult time achieving the speedups I needed because of the expensive process of generating the transposed matrix. I instead used a method that allowed us to vectorize the operations with SIMD instructions. I did this by performing an equivalent method of matrix multiplication, instead multiplying single elements from matrix A with rows of matrix B and then accumulating the results. Since I were storing my matrix data as 1 dimensional arrays in row major order, this method of multiplying each row of the second matrix by scalar elements from the first matrix improved the spatial locality of my cache accesses without the overhead of transposition operations. On top of using SIMD instructions, I also added OpenMP parallelization and reordered the loops to be in ikj order to improve the cache hit rate. Unrolling what I had yielded another massive performance boost, but unfortunately this wasn't good enough to improve my pow score. I saw on Piazza that the pow function uses smaller matrices, so I have two separate mul functions, one of which is used exclusively for pow and does not include as much optimization overhead.

**Update for multiplication**: As it did for pow, the overhead of pointer arithmetic for 1d row-major order indexing causes a significant performance hit. I mitigated this by using 2d array indexing and storing values in temp variables if they are repeatedly used.

### Exponentiation
For pow, I optimize by reducing the number of matrix multiplications I perform. Using a method taught in cs70, exponentiation by squaring, I were able to reduce the number of multiplications from O(n) to O(logn) -- n being the exponent. This optimization was enough to achieve only a ~500x speedup, so I added some additional speedups. my original method required us to copy matrix data into temporary matrices to store intermediate values and avoid self referencing in the mul function. my new approach involved swapping only the pointers to my temporary matrices, avoiding the need to make copies of the matrix data until one sweep at the very end. While this led to some small speedups, further experimentation revealed that approximately 90% of my pow time was consumed by mul operations while only ~10% of the time was spent copying matrix data. To further speed up the process, I created a new allocate function that skipped 2d pointer allocation and error checks on matrix dimensions since (which was safe to do since this function is used only in pow) as well as a new mul function that skips error checks. I also used OpenMP to parallelize the creation of an identity matrix at the beginning of the function as well as parallelizing the process of copying data between matrices. For matrices with exponents of 0 and 1, I added base cases to prevent unnecessary computations. Unfortunately, this led to a speedup of only ~970x. I thought it was bottlenecked mostly by my mul function, so I tried many different variations of my mul approach, but most of them slowed my pow function even more. I did manage to significantly speed up my mul function using unrolling and regrouping instructions for better cache usage, but, to my disappointment, this optimization actually led to a significant reduction in performance when used with my pow function. I ended up resetting back to my ~970x speedup code with my old secondary mul function for pow and my new unrolled mul function as the primary one. I realized that I needed to completely rethink my mul approach for small matrices if I wanted to meet the pow benchmark.

**Update for exponentiation**: I discovered a bug in my pow function that fails for large test cases. For exponentiation to large powers of 2 (256, 512, etc), my repeated squaring would end on a multiplication with the identity matrix. While this is fine under normal conditions, it breaks when the values in my matrix become inf. This is because inf * 0 returns a NaN value. I realized upon fixing this issue that I didn't really need to generate an identity matrix in the first place, unless the exponent was 0. Instead, I used a boolean value to represent whether the result value should hold identity matrix values or not, and then copied elements if it was (instead of running the multiply function) since any matrix A * I = A. This both fixed my bug and had a nice side effect of saving us the need to do some multiplication operations. I also made some minor optimizations with deallocation and allocation since I could make some assumptions about my temporary matrices and skip some checks. I also realized that I could entirely avoid the need for copying matrix values (other than what's done inside the mul function). At the very end of my pow function, I swap the data pointers from my temp matrix with my result matrix instead of copying the data over.

I also discovered the issue that was bottlenecking my pow performance. In an attempt to optimize my mul function for pow, I used arrays to hold intermediate values instead of allocating matrix structs. What I didn't take into account was that the arithmetic for indexing into row-major order arrays was huge proportional to the total time it takes to perform exponentiation on small matrices. Switching to allocating matrices and using 2d indexing immediately improved my performance. The single most important boost to my power performance was the realization that naive ikj loop ordering with OpenMP was much better for smaller matrices than my SIMD approach because it has less overhead. Unrolling the inner k loop and adding conditions for small matrices showed promise as well. I pivoted to this method, which got us to around 1500-1600x range on the pow benchmark. I further boosted it into the 1700x-1800x by tweaking my conditional statements for the size cutoffs to start using OpenMP and unrolling vs just using the naive ikj loop ordering.

**Update 2 for exponentiation**: Setting my thread count to 4 for pow boosted my score to over 3000x! I set it back to the default of 8 before each return statement to prevent performance hits on larger matrices.
